# Multi-Turn Jailbreaks Are Simpler Than They Seem

This repository contains the code and data for our paper "Multi-Turn Jailbreaks Are Simpler Than They Seem", accepted at the SoLaR (Society, Language, and Reasoning) workshop at NeurIPS 2025.

## Abstract

While defenses against single-turn jailbreak attacks on Large Language Models (LLMs) have improved significantly, multi-turn jailbreaks remain a persistent vulnerability, often achieving success rates exceeding 70% against models optimized for single-turn protection. This work presents an empirical analysis of automated multi-turn jailbreak attacks across state-of-the-art models including GPT-4, Claude, and Gemini variants, using the StrongREJECT benchmark. Our findings challenge the perceived sophistication of multi-turn attacks: when accounting for the attacker's ability to learn from how models refuse harmful requests, multi-turn jailbreaking approaches are approximately equivalent to simply resampling single-turn attacks multiple times. Moreover, attack success is correlated among similar models, making it easier to jailbreak newly released ones. Additionally, for reasoning models, we find surprisingly that higher reasoning effort often leads to higher attack success rates.

## Repository Structure

```
├── main.py                 # Main entry point for running attacks
├── utils/                  # Utility functions for generation and evaluation
├── jailbreaks/            # Jailbreak tactics implementation
│   └── direct_request/    # Direct Request tactic (main focus)
├── test_cases/            # 30 harmful behaviors from StrongREJECT
├── csv_results/           # Experimental results
│   └── results_strongreject.csv  # Main results file
├── plot_scripts/          # Scripts for generating paper figures
├── figure_generation/     # Actual scripts that generated paper figures
│   ├── create_custom_asr_figure_batch7.py
│   ├── batch7_correlation_analysis.py
│   └── reasoning_token_analysis.py
└── figures/               # Paper figures for reference
```

## Installation

```bash
# Create virtual environment (optional but recommended)
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# For figure generation, also install:
pip install -r requirements-plotting.txt
```

## Reproducing Paper Figures

The main results used in our paper are stored in `csv_results/results_strongreject.csv`. Below are detailed instructions for reproducing each figure from the paper.

### Figure 1: Pipeline Overview (figures/pipeline.png)
This is a conceptual diagram created manually outside of the codebase.

### Figure 2: Single-Turn vs Multi-Turn Attack Success Rates
**Original filename**: `placeholder_single_vs_multi_7_2.pdf`  
**Generated by**: `figure_generation/create_custom_asr_figure_batch7.py`

This figure shows the comparison between single-turn and multi-turn attack success rates across different models with special handling for refusal cases.

```bash
cd figure_generation
python3 create_custom_asr_figure_batch7.py

# Output: custom_asr_figure_batch7.pdf
```

**Note**: This script requires batch7 JSONL data files. If you don't have the raw data, you can use the preprocessed CSV results.

### Figure 3: Attack Success vs Number of Samples/Turns
**Original filename**: `placeholder_samples_turns.pdf`  
**Generated by**: Analysis notebooks (asr_rounds_samples_comparison.ipynb)

This figure demonstrates how attack success increases with additional turns or resampling attempts. The exact generation requires running Jupyter notebooks with the full experimental data.

### Figure 4: Attack Success Rates for Reasoning Models
**Original filename**: `placeholder_reasoning_bars.pdf`  
**Generated by**: `figure_generation/reasoning_token_analysis.py`

This figure shows the relationship between reasoning token usage and attack success rates.

```bash
cd figure_generation
python3 reasoning_token_analysis.py --input ../csv_results/results_strongreject.csv

# Output: Files with reasoning analysis results
```

### Appendix Figures: Correlation Matrices
**Original filenames**: `placeholder_correlation_single.pdf`, `placeholder_correlation_multi.pdf`  
**Generated by**: `figure_generation/batch7_correlation_analysis.py`

These figures show the correlation of attack success across different models for single-turn and multi-turn attacks.

```bash
cd figure_generation
# Note: This script expects data at specific paths. You may need to modify paths in the script.
python3 batch7_correlation_analysis.py

# Outputs:
# - batch7_single_turn_correlations.pdf
# - batch7_multi_turn_correlations.pdf
# - batch7_single_turn_correlation_matrix.csv
# - batch7_multi_turn_correlation_matrix.csv
```

### Using the Plot Scripts

The `plot_scripts/` directory contains additional visualization scripts from the original repository:

```bash
cd plot_scripts
python3 plot_main_graphs_from_csv.py --input ../csv_results/results_strongreject.csv
```

## Running New Experiments

### Single Attack Example
```bash
python main.py --jailbreak-tactic "direct_request" --test-case "counterfeit_money" --target-model "gpt-4" --attacker-model "gpt-4o-mini"
```

### Parameters
- `--jailbreak-tactic`: Attack strategy to use (e.g., "direct_request")
- `--test-case`: Harmful behavior to test (see test_cases/ directory)
- `--target-model`: Model to attack
- `--attacker-model`: Model generating attack prompts
- `--turn-type`: "single" or "multi" (default: "multi")
- `--n-samples`: Number of times to run the attack (default: 1)

### API Keys
You'll need to set up your API keys in a `.env` file:
```bash
cp .env.example .env
# Edit .env with your OpenRouter API key
```

## Key Findings

1. **Multi-turn attacks are more effective than single-turn attacks** across all tested models
2. **The advantage of multi-turn attacks can be largely explained by additional sampling opportunities** rather than sophisticated conversational strategies
3. **Attack success correlates among similar models**, suggesting transferability of vulnerabilities
4. **Reasoning models show higher vulnerability** when using more reasoning tokens

## Citation

If you use this code in your research, please cite:

```bibtex
@inproceedings{cruz2025multiturn,
  title={Multi-Turn Jailbreaks Are Simpler Than They Seem},
  author={[Authors]},
  booktitle={SoLaR Workshop at NeurIPS 2025},
  year={2025},
  url={https://solar-colm.github.io/}
}
```

## Ethical Considerations

This tool is intended for research and educational purposes only. It should be used responsibly to improve AI safety, not to cause harm. Please use in accordance with applicable laws and regulations.

## License

MIT License - see LICENSE file for details.

## Acknowledgments

This work was conducted as part of the AI Safety Camp. We thank the SoLaR workshop organizers and the NeurIPS community.